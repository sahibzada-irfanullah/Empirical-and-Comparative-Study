{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "# import re\n",
    "# import scipy\n",
    "# import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_model(path):\n",
    "    if \"Model\" not in path:\n",
    "        print(\"You may not loading a model; You may loading a tokenizer.\")\n",
    "    if \"glove\".lower() in path.lower().split(\"/\")[-1]:\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "\n",
    "            words = set()\n",
    "            # list(tokenizer.word_index.keys())\n",
    "            word_to_vec_map = {}\n",
    "            for line in f:\n",
    "                w_line = line.split()\n",
    "                curr_word = w_line[0]\n",
    "                if curr_word in list(tokenizer.word_index.keys()):\n",
    "                    word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "    elif \"word2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "        word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "    elif \"google\".lower() in path.lower().split(\"/\")[-1]:\n",
    "\n",
    "        word_to_vec_map = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    elif \"doc2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "        word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "    return word_to_vec_map\n",
    "\n",
    "def word_vector_avg(inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "    vec = np.zeros(size)\n",
    "    count = 0\n",
    "    columnNameList = []\n",
    "    for i in range(18):\n",
    "        columnName = \"rule\" + str(i + 1)\n",
    "        columnNameList.append(columnName)\n",
    "    for seq in tw_sequence:\n",
    "        try:\n",
    "            if inv_tokenizer[seq] in columnNameList:\n",
    "                embedding_vector = np.ndarray(shape=(size,))\n",
    "                embedding_vector[:] = 1/size\n",
    "            else:\n",
    "                embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "            if embedding_vector is not None:\n",
    "                vec += embedding_vector\n",
    "            else:\n",
    "                vec += np.zeros(size)\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may not loading a model; You may loading a tokenizer.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word_to_vec_map' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24324/2850928304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpathToWord2vecModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_vect/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileNameWord2vecModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfun_word_to_vec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word2vec_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToWord2vecModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfun_word_to_vec_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24324/3516205781.py\u001b[0m in \u001b[0;36mload_word2vec_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"doc2vec\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mword_to_vec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_vector_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun_word_to_vec_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'word_to_vec_map' referenced before assignment"
     ]
    }
   ],
   "source": [
    "fileNameWord2vecModel = \"22-05-06_20-59-32_Unigrams_CountVect_WRul_Freq-3803\"\n",
    "pathToWord2vecModel = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_vect/\" + fileNameWord2vecModel\n",
    "\n",
    "fun_word_to_vec_map = load_word2vec_model(pathToWord2vecModel)\n",
    "fun_word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gen_rules_features(X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "    X_Rules_dtm, features_Rules = gen_rules_features(X, tokenizer)\n",
    "    X_train_WR_dtm, combined_features = concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                      X_train_WoR_Features, features_Rules)\n",
    "    return X_train_WR_dtm, combined_features\n",
    "\n",
    "def gen_rules_features(X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "    '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "it as features...\n",
    "I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "  for changing datatypes'''\n",
    "    X_data_DF = series_to_DataFrame(X_data_series)\n",
    "    regexes = [\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                   re.I | re.M),\n",
    "        re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                   re.I | re.M),\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "        re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "        re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "    ]\n",
    "    temp = pd.DataFrame()\n",
    "    features_arg = []\n",
    "    for i, regex in zip(range(len(regexes)), regexes):\n",
    "        # we can also use ruleString()\n",
    "        columnName = \"rule\" + str(i + 1)\n",
    "        features_arg.append(columnName)\n",
    "        temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: apply_regex(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "    temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "    return temp_sparse, features_arg\n",
    "\n",
    "def series_to_DataFrame(X_data):\n",
    "    X_data = X_data.to_frame()\n",
    "    return X_data\n",
    "\n",
    "def apply_regex(text, regex, ruleTokenizerSequenceIndex):\n",
    "    match_found = (re.search(regex, text) != None)\n",
    "    if match_found:\n",
    "        match_found = int(ruleTokenizerSequenceIndex)\n",
    "    else:\n",
    "        match_found = 0\n",
    "    return match_found\n",
    "\n",
    "\n",
    "def concat_sparse_matrices_h(data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "    combined_features = features_X + features_Rules\n",
    "    concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "    return concat_sparse, combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/datasets/preprocessed_500_random_sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b594c92f8805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mpath_to_nonPreprocessedDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/datasets/500_random_sample.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0minclude_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"yes\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnameTrainingDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m X_train_dtm = word2vec_Vect(pathToTokenizer, pathToWord2vecModel, df,\\\n\u001b[0;32m     66\u001b[0m                                                     path_to_nonPreprocessedDataset, include_rules)    \n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RweetMiner\\RweetMinerVenv\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/datasets/preprocessed_500_random_sample.csv'"
     ]
    }
   ],
   "source": [
    "def invTokenizer(tokenizer):\n",
    "    inv_tokenizer = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    return inv_tokenizer\n",
    "\n",
    "def word2vec_Vect(pathToTokenizer, pathToWord2vecModel, df, path_to_nonPreprocessedDataset = \"\", include_rules=False):\n",
    "    tokenizer = pickle.load(open(pathToTokenizer, 'rb'))\n",
    "    embed_dim = int(pathToTokenizer.split(\"Freq-\")[1])\n",
    "    if \"WRul\" in pathToTokenizer:\n",
    "        MAX_SEQUENCE_LENGTH = int(pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0]) - 18\n",
    "    else:\n",
    "        MAX_SEQUENCE_LENGTH = int(pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0])\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(df['tweet_text'])\n",
    "    X_train_WoR_dtm = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_train_Features = [\"_Not_Apply_\"]\n",
    "    print(type(X_train_WoR_dtm), X_train_WoR_dtm.shape)\n",
    "    if include_rules == 'yes':\n",
    "        df_notPreprocessed = pd.read_csv(path_to_nonPreprocessedDataset)\n",
    "        X_train_dtm, _ = apply_gen_rules_features(df_notPreprocessed[\"tweet_text\"], X_train_WoR_dtm, tokenizer, \\\n",
    "                                                  X_train_Features)\n",
    "    else:\n",
    "        X_train_dtm = X_train_WoR_dtm\n",
    "        X_train_dtm = np.unique(X_train_dtm, axis=0)\n",
    "    print(type(X_train_dtm), X_train_dtm.shape)\n",
    "    inv_tokenizer = invTokenizer(tokenizer)\n",
    "    fun_word_to_vec_map = load_word2vec_model(pathToWord2vecModel)\n",
    "    \n",
    "   \n",
    "    vector = []\n",
    "    for tw_sequence in X_train_dtm.toarray():\n",
    "        vec = word_vector_avg(inv_tokenizer, fun_word_to_vec_map, tw_sequence, embed_dim)\n",
    "        vector.append(vec)\n",
    "    X_train_dtm = np.array(vector)\n",
    "    print(50*\"*\")\n",
    "    print(\"Evaluation\")\n",
    "    if \"WRul\" in pathToTokenizer:\n",
    "        if include_rules.lower() == \"no\":\n",
    "            print(\"Vectoirzer and appending does not match\")\n",
    "    if \"WoRul\" in pathToTokenizer:\n",
    "        if include_rules.lower() == \"yes\":\n",
    "            print(\"Vectoirzer and appending does not match\")\n",
    "            \n",
    "    if \"WRul\" in pathToWord2vecModel:\n",
    "        if include_rules.lower() == \"no\":\n",
    "            print(\"Word2vec Model and appending does not match\")\n",
    "    if \"WoRul\" in pathToWord2vecModel:\n",
    "        if include_rules.lower() == \"yes\":\n",
    "            print(\"Word2vec Model and appending does not match\")\n",
    "    print(50*\"*\")\n",
    "    return X_train_dtm\n",
    "  \n",
    "    \n",
    "\n",
    "    \n",
    "filenameTokenizer = \"22-04-15_09-10-29_word2vec_Vect_WRul_Max-Len-Seq-38_Freq-300\"\n",
    "pathToTokenizer = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_vect/\" + filenameTokenizer\n",
    "\n",
    "fileNameWord2vecModel = \"22-04-15_09-10-29_word2vec_Model_WRul_Max-Len-Seq-38_Freq-300\"\n",
    "pathToWord2vecModel = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_vect/\" + fileNameWord2vecModel\n",
    "\n",
    "fnameTrainingDataset = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/datasets/preprocessed_500_random_sample.csv\"\n",
    "path_to_nonPreprocessedDataset = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/datasets/500_random_sample.csv\"\n",
    "include_rules = \"yes\"\n",
    "df = pd.read_csv(fnameTrainingDataset)\n",
    "X_train_dtm = word2vec_Vect(pathToTokenizer, pathToWord2vecModel, df,\\\n",
    "                                                    path_to_nonPreprocessedDataset, include_rules)    \n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadClassificationModel(pathToClfModel):\n",
    "    clf_nb = pickle.load(open(pathToClfModel, 'rb'))\n",
    "    return clf_nb\n",
    "\n",
    "fileNameClfModel = \"22-04-14_11-27-52_Vect_WRul_Max-Len-Seq-20_Freq-318_22-04-14_11-27-52_word2vec_Vect_WRul_Max-Len-Seq-20_Freq-318_NB\"\n",
    "pathToClfModel = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_clfModels/\" + fileNameClfModel\n",
    "clf_nb = loadClassificationModel(pathToClfModel)\n",
    "clf_nb.predict(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnameTrainingDataset = \"datasets/200_random_sample.csv\"\n",
    "df = pd.read_csv(fnameTrainingDataset)\n",
    "X = df['tweet-text']\n",
    "y_binary = df['tweet-class']\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_binary = le.fit_transform(y_binary)\n",
    "\n",
    "classes_labels_binary = le.classes_\n",
    "sequences = load_vect.texts_to_sequences(X)\n",
    "X_train_WoR_dtm = pad_sequences(sequences, maxlen=38)\n",
    "X_train_WoR_Features = list(load_vect.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 14  3 14 14]\n",
      " [ 1 14  3 14 14]\n",
      " [ 3 13  1 13 13]\n",
      " [ 7 12  9 12 12]\n",
      " [ 9 11  9 11 11]\n",
      " [ 7 12  9 12 12]\n",
      " [ 7 10  9 10 10]]\n",
      "(7, 5)\n",
      "[[ 1 14  3 14 14]\n",
      " [ 3 13  1 13 13]\n",
      " [ 7 10  9 10 10]\n",
      " [ 7 12  9 12 12]\n",
      " [ 9 11  9 11 11]]\n",
      "(5, 5)\n",
      "[0 2 6 3 4]\n",
      "[ 1  3 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create numpy arrays\n",
    "data = np.array([[1, 14, 3, 14, 14],\n",
    "                 [1, 14, 3, 14, 14],\n",
    "                 [3, 13, 1, 13, 13],\n",
    "                 [7, 12, 9, 12, 12],\n",
    "                 [9, 11, 9, 11, 11],\n",
    "                 [7, 12, 9, 12, 12],\n",
    "                 [7, 10, 9, 10, 10]])\n",
    "# Remove Duplicate columns from 2D NumPy Array\n",
    "print(data)\n",
    "print(data.shape)\n",
    "data_uniq, indice_uniq = np.unique(data, return_index=True, axis=0)\n",
    "print(data_uniq)\n",
    "print(data_uniq.shape)\n",
    "print(indice_uniq)\n",
    "print(np.take(data, indice_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 14,  3, 14, 14],\n",
       "       [ 1, 14,  3, 14, 14],\n",
       "       [ 3, 13,  1, 13, 13],\n",
       "       [ 7, 12,  9, 12, 12],\n",
       "       [ 9, 11,  9, 11, 11],\n",
       "       [ 7, 12,  9, 12, 12],\n",
       "       [ 7, 10,  9, 10, 10]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vecModels/crisisNLP_word_vector.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-ed59dc497fad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'word2vecModels/crisisNLP_word_vector.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# model = word2vec.Word2Vec.load_word2vec_format('../word2vecModels/crisisNLP_word_vector.bin', binary=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shelter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1629\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m         )\n\u001b[0;32m   1633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vecModels/crisisNLP_word_vector.bin'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "path = 'word2vecModels/crisisNLP_word_vector.bin'\n",
    "# model = word2vec.Word2Vec.load_word2vec_format('../word2vecModels/crisisNLP_word_vector.bin', binary=True)\n",
    "model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "words=model.most_similar(positive=['shelter'], negative=[], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"22-04-14_09-37-50_word2vec_Vect_WRul_Max-Len-Seq-38_Freq-318\"\n",
    "path = \"F:/RweetMiner/RweetMinerNew/request_identification_class_DLCorrected/word2vec_vect/\" + filename\n",
    "load_model = pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regex(text, regex):\n",
    "    match_found = (re.search(regex ,text) !=None)\n",
    "    match_found = int(match_found == True)\n",
    "    return match_found\n",
    "\n",
    "def series_to_DataFrame(X_data):\n",
    "    X_data = X_data.to_frame()\n",
    "    return X_data\n",
    "\n",
    "def concat_sparse_matrices_h(data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "    combined_features = features_X + features_Rules\n",
    "    concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "    return concat_sparse, combined_features\n",
    "\n",
    "def gen_rules_features(X_data_series): # , X_data_dtm, features_arg):\n",
    "    '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "    X_data_DF = series_to_DataFrame(X_data_series)\n",
    "#     X_data_DF = X_data_series\n",
    "    regexes = [\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I\\'m)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'re)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'ll)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b\\w*\\s*\\b\\?', re.I|re.M),\n",
    "      re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M)\n",
    "\n",
    "    ]\n",
    "    temp = pd.DataFrame()\n",
    "    features_arg = []\n",
    "    for i, regex in zip(range(len(regexes)), regexes):\n",
    "      columnName = \"RegEx_\" + str(i + 1)\n",
    "      features_arg.append(columnName)\n",
    "      temp[columnName] = X_data_DF['tweet-text'].apply(lambda text: apply_regex(text, regex))\n",
    "    temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "    return temp_sparse, features_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d50522026ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclasses_labels_binary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mX_train_WoR_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mX_train_WoR_Features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "fnameTrainingDataset = \"datasets/200_random_sample.csv\"\n",
    "df = pd.read_csv(fnameTrainingDataset)\n",
    "X = df['tweet-text']\n",
    "y_binary = df['tweet-class']\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_binary = le.fit_transform(y_binary)\n",
    "\n",
    "classes_labels_binary = le.classes_\n",
    "X_train_WoR_dtm = load_vect.transform(X)\n",
    "X_train_WoR_Features = list(load_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Rules_dtm, features_Rules = gen_rules_features(X)\n",
    "X_train_WR_dtm, combined_features = concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm, X_train_WoR_Features, features_Rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_bin_pred = load_model.predict(X_train_WR_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['DT'] = y_bin_pred.tolist()\n",
    "df['GB'] = np.random.randint(2, size=500).tolist()\n",
    "df['MLP'] = np.random.randint(2, size=500).tolist()\n",
    "df['RF'] = np.random.randint(2, size=500).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"request\"] = df.iloc[:, -4:].mean(axis=1) > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DT</th>\n",
       "      <th>GB</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DT  GB  MLP  RF\n",
       "0     0   0    1   0\n",
       "1     1   0    1   1\n",
       "2     0   1    1   1\n",
       "3     0   1    1   1\n",
       "4     0   1    0   0\n",
       "..   ..  ..  ...  ..\n",
       "495   1   0    0   1\n",
       "496   0   0    1   1\n",
       "497   0   0    1   1\n",
       "498   0   1    0   1\n",
       "499   0   0    0   1\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "e = np.empty([0])\n",
    "e = np.append(e, 1)\n",
    "e = np.append(e, 2)\n",
    "e = np.append(e, 3)\n",
    "print(list(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [2, 2],\n",
       "       [3, 0],\n",
       "       [0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GB</th>\n",
       "      <th>MLP</th>\n",
       "      <th>RF</th>\n",
       "      <th>num_uniq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GB  MLP  RF  num_uniq\n",
       "0     0    1   1         4\n",
       "1     0    0   0         4\n",
       "2     0    0   0         4\n",
       "3     1    1   0         4\n",
       "4     1    1   1         4\n",
       "..   ..  ...  ..       ...\n",
       "495   0    0   0         4\n",
       "496   1    0   1         4\n",
       "497   0    0   0         4\n",
       "498   0    0   0         4\n",
       "499   0    0   1         4\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "[1. 2. 3. 0.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1  c2  c3  c4  new\n",
       "0   1   1   1   0  1.0\n",
       "1   2   1   2   2  2.0\n",
       "2   2   3   3   0  3.0\n",
       "3   2   0   0   0  0.0"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_from_rows_into_column(df, indexes):\n",
    "    \"\"\" The function takes dataframe and index of columns, and \n",
    "    then it returns the array containg the most frequent value against each row.\n",
    "    Example: \n",
    "    dataframe:\n",
    "    'c1' | 'c2'| 'c3' | 'c4'\n",
    "    -------------------------\n",
    "      1  |  1  |   1  |  0  \n",
    "      2  |  1  |   2  |  2\n",
    "      2  |  3  |   3  |  0 \n",
    "      2  |  0  |   0  |  0 \n",
    "      returned array:\n",
    "      (1, 2, 3, 0)\n",
    "      df: dataframe\n",
    "      indexes: indexes of columns\n",
    "      example: get_max_from_rows_into_column(dataframe, -3:)\n",
    "      \"\"\"\n",
    "    def get_array(df):\n",
    "        max_unique_label_column = np.empty([0])\n",
    "        for v in df.values:\n",
    "            unique_label, freq_label = np.unique(v, return_counts=True)\n",
    "            print(unique_label[np.argmax(freq_label)])\n",
    "            max_unique_label_column = np.append(max_unique_label_column, unique_label[np.argmax(freq_label)])\n",
    "            \n",
    "#             print(unique_label[np.argmax(freq_label)])\n",
    "        return max_unique_label_column\n",
    "    if \":\" in indexes:\n",
    "        if indexes[0] == \":\":\n",
    "            \n",
    "            df = df.iloc[:, :int(indexes[1:])]\n",
    "            max_unique_label_column = get_array(df)\n",
    "        if indexes[-1] == \":\":\n",
    "            df = df.iloc[:, int(indexes[:-1]):]\n",
    "            max_unique_label_column = get_array(df)\n",
    "           \n",
    "        else:\n",
    "            indexes = [int(x) for x in indexes.split(\":\")]\n",
    "            df = df.iloc[:, indexes[0]: indexes[1]]\n",
    "            max_unique_label_column = get_array(df)\n",
    "    else:\n",
    "            indexes = [int(x) for x in indexes.split(\",\")]\n",
    "            df = df.iloc[:, indexes]\n",
    "            max_unique_label_column = get_array(df)\n",
    "    print(max_unique_label_column)\n",
    "    return max_unique_label_column\n",
    "#     for v in df.iloc[:, indexes].values:\n",
    "#         unique_label, freq_label = np.unique(v, return_counts=True)\n",
    "#         print(unique_label)\n",
    "#         print(freq_label)\n",
    "#         print(unique_label[np.argmax(freq_label)])\n",
    "#     #     print(counts.indexOf(max(counts)))\n",
    "    #     print(unique)\n",
    "\n",
    "#         print(40*\"*\")\n",
    "\n",
    "d = {'c1': [1, 2, 2, 2], 'c2': [1, 1, 3, 0], 'c3': [1, 2, 3, 0], 'c4': [0, 2, 0, 0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "indexes = \"-3:\"\n",
    "df[\"new\"] = get_max_from_rows_into_column(df, indexes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4:-3\n",
      "[-4, -3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DT</th>\n",
       "      <th>GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DT  GB\n",
       "0     0   0\n",
       "1     1   0\n",
       "2     0   1\n",
       "3     0   1\n",
       "4     0   1\n",
       "..   ..  ..\n",
       "495   1   0\n",
       "496   0   0\n",
       "497   0   0\n",
       "498   0   1\n",
       "499   0   0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = \"-4:-3\"\n",
    "print(indexes)\n",
    "if \":\" in indexes:\n",
    "    if indexes[0] == \":\" or indexes[-1] == \":\":\n",
    "        indexes = indexes\n",
    "    else:\n",
    "        indexes = [int(x) for x in indexes.split(\":\")]\n",
    "        return\n",
    "else:\n",
    "    indexes = [int(x) for x in indexes.split(\",\")]\n",
    "print(indexes)\n",
    "df.iloc[:, indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1  c2  c3  c4\n",
       "0   1   1   1   0\n",
       "1   2   1   2   0\n",
       "2   2   3   2   0\n",
       "3   2   0   0   0"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'c1': [1, 2, 2, 2], 'c2': [1, 1, 3, 0], 'c3': [1, 2, 2, 0], 'c4': [0, 2, 0, 0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x2600 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2600 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "fnameTrainingDataset = \"datasets/500_random_sample.csv\"\n",
    "df = pd.read_csv(fnameTrainingDataset)\n",
    "X = df['tweet_text']\n",
    "Vect = CountVectorizer(ngram_range=(2,2), min_df=1, max_df=1)\n",
    "Vect.fit(X)\n",
    "X_train_WoR_dtm = Vect.transform(X)\n",
    "X_train_WoR_dtm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virEnv375",
   "language": "python",
   "name": "virenv375"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6556751-28e8-47b9-81b5-6f5cff23e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import scipy\n",
    "import numpy as np\n",
    "import glob\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b6204a69-649d-4341-8149-a9da332f5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UsePretrainedVectClf:\n",
    "    def __init__(self, pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "        self.path_to_PreprocessedDataset = path_to_PreprocessedDataset\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "    \n",
    "    def traditional_ChangeVectPath(self, pathToTokenizer):\n",
    "        self.pathToTokenizer = pathToTokenizer   \n",
    "        \n",
    "    def traditional_ChangeClfModelPath(self, pathToClfModel):\n",
    "        self.pathToClfModel = pathToClfModel     \n",
    "        \n",
    "    def traditional_loadVect(self):\n",
    "        Vect = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        return Vect\n",
    "    \n",
    "    def apply_gen_rules_features_ngrams(self, X, X_train_WoR_dtm, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_ngrams(X)\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_features\n",
    "\n",
    "\n",
    "    def gen_rules_features_ngrams(self, X_data_series):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "        it as features...\n",
    "        I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "          for changing datatypes'''\n",
    "        \n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            columnName = \"RegEx_\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_ngrams(text, regex))\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "        return temp_sparse, features_arg\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def apply_regex_ngrams(self, text, regex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        match_found = int(match_found == True)\n",
    "        return match_found\n",
    "    \n",
    "    def traditional_getFeatures(self, Vect):\n",
    "        df = pd.read_csv(self.path_to_PreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        X_train_WoR_dtm = Vect.transform(df[\"tweet_text\"])\n",
    "        X_train_Features = list(Vect.get_feature_names_out())\n",
    "        if self.include_rules:\n",
    "            df_notPreprocessed = pd.read_csv(self.path_to_nonPreprocessedDataset,  encoding = \"ISO-8859-1\")\n",
    "            X_train_dtm, X_train_Features = self.apply_gen_rules_features_ngrams(df_notPreprocessed[\"tweet_text\"], \\\n",
    "                                                                      X_train_WoR_dtm, X_train_Features)\n",
    "        else:\n",
    "            X_train_dtm = X_train_WoR_dtm\n",
    "        return X_train_dtm.toarray(), Vect\n",
    "\n",
    "    def traditional_loadClf(self):  \n",
    "        clf_model = pickle.load(open(self.pathToClfModel, 'rb'))\n",
    "        return clf_model\n",
    "    \n",
    "    def get_predictions(self, clf_model, X):\n",
    "        return clf_model.predict(X)\n",
    "    \n",
    "    def generateNameFromDataSetName(self, list1):\n",
    "        str1 = \"\"\n",
    "        for e in list1:\n",
    "            if e == \"\":\n",
    "                str1 += '.'.join(str(e))\n",
    "            else:\n",
    "                if e == \"csv\":\n",
    "                    e = \"_withPredictions.\" + e\n",
    "                    str1 += ''.join(str(e))\n",
    "                else:\n",
    "                    str1 += ''.join(str(e))\n",
    "        return \"..\" + str1        \n",
    "    \n",
    "    def evaluation(self):\n",
    "        if \"WRul\" in self.pathToTokenizer:\n",
    "            if not self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WoRul\" in self.pathToTokenizer:\n",
    "            if self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WRul\" in self.pathToClfModel:\n",
    "            if not self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WoRul\" in self.pathToClfModel:\n",
    "            if self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if not self.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:-1] == self.pathToTokenizer.split(\"/\")[-1].split(\"_\"):\n",
    "            print(\"Vectorizer and classifer does not match.\")\n",
    "            print(\"Vectorizer:\", self.pathToTokenizer)\n",
    "            print(\"Classifier:\", self.pathToClfModel)\n",
    "            print(\"Classifier:\", self.pathToTokenizer.split(\"/\")[-1].split(\"_\"), \"Vectorizer:\" , self.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:-1])\n",
    "\n",
    "    def generatePredNGramsPretrained(self, path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                         pathToClfModel, path2word2vecModel, pathToWord2VecPretrained):\n",
    "\n",
    "        NotProcessedDataWithPredictions = pd.read_csv(obj.path_to_nonPreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        for vect_filename in glob.glob(pathToVectFolder + \"/*\"):\n",
    "            print(40*\"+\")\n",
    "            obj.traditional_ChangeVectPath(vect_filename)\n",
    "            Vect = obj.traditional_loadVect()\n",
    "            for clfModel_filename in glob.glob(pathToClfFolder + \"*\"):\n",
    "                if clfModel_filename.split(\"/\")[-1].split(\"_\")[5:-1] == vect_filename.split(\"/\")[-1].split(\"_\"):\n",
    "                    obj.traditional_ChangeClfModelPath(clfModel_filename)\n",
    "                    name_cell = obj.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:]\n",
    "                    if name_cell[-3] == \"WRul\":\n",
    "                        obj.include_rules = True\n",
    "                    else:\n",
    "                        obj.include_rules = False\n",
    "                    # print(obj.pathToTokenizer, obj.pathToClfModel)\n",
    "                    # print(name_cell)\n",
    "                    # print(name_cell[-5] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1])\n",
    "                    X, _ = obj.traditional_getFeatures(Vect)\n",
    "                    clf_model = obj.traditional_loadClf()\n",
    "                    y_pred = obj.get_predictions(clf_model, X)\n",
    "                    name_cell = name_cell[-5] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1]\n",
    "                    print(name_cell)\n",
    "                    NotProcessedDataWithPredictions[name_cell] = y_pred.tolist()\n",
    "        notProcessedDatasetName = obj.generateNameFromDataSetName(obj.path_to_nonPreprocessedDataset.split(\".\"))\n",
    "        NotProcessedDataWithPredictions.to_csv(notProcessedDatasetName, index=False)\n",
    "        print(40*\"-\")\n",
    "        \n",
    "    def word_vector_avg(self, inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "        vec = np.zeros(size)\n",
    "        count = 0\n",
    "        columnNameList = []\n",
    "        for i in range(18):\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            columnNameList.append(columnName)\n",
    "        for seq in tw_sequence:\n",
    "            try:\n",
    "                if inv_tokenizer[seq] in columnNameList:\n",
    "                    embedding_vector = np.ndarray(shape=(size,))\n",
    "                    embedding_vector[:] = 1/size\n",
    "                else:\n",
    "                    embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "                if embedding_vector is not None:\n",
    "                    vec += embedding_vector\n",
    "                else:\n",
    "                    vec += np.zeros(size)\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "   \n",
    "    \n",
    "    \n",
    "    def apply_regex_word2vec(self, text, regex, ruleTokenizerSequenceIndex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        if match_found:\n",
    "            match_found = int(ruleTokenizerSequenceIndex)\n",
    "        else:\n",
    "            match_found = 0\n",
    "        return match_found\n",
    "    \n",
    "    def gen_rules_features_word2vec(self, X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            # we can also use ruleString()\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_word2vec(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "\n",
    "        return temp_sparse, features_arg\n",
    "    \n",
    "    def apply_gen_rules_features_word2vec(self, X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_word2vec(X, tokenizer)\n",
    "\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_feature\n",
    "    \n",
    "\n",
    "    def word2vec_ChangeWord2VecPretrainedPath(self, pathToWord2VecPretrained):\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained \n",
    "\n",
    "    def word2vec_loadWord2VecPretrained(self):  \n",
    "        word2vec_model = pickle.load(open(self.pathToWord2VecPretrained, 'rb'))\n",
    "        return word2vec_model\n",
    "    \n",
    "\n",
    "\n",
    "    def invTokenizer(self, tokenizer):\n",
    "        inv_tokenizer = {v: k for k, v in tokenizer.word_index.items()}\n",
    "        return inv_tokenizer\n",
    "\n",
    "    def word2vec_Vect(self, pathToWord2VecPretrained, df):\n",
    "        tokenizer = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        embed_dim = int(self.pathToTokenizer.split(\"Freq-\")[1])\n",
    "        if \"WRul\" in self.pathToTokenizer:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0]) - 18\n",
    "        else:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0])\n",
    "\n",
    "        sequences = tokenizer.texts_to_sequences(df['tweet_text'])\n",
    "        X_train_WoR_dtm = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "   \n",
    "        X_train_Features = [\"_Not_Apply_\"]\n",
    "        # print(type(X_train_WoR_dtm), X_train_WoR_dtm.shape)\n",
    "        if self.include_rules:\n",
    "            df_notPreprocessed = pd.read_csv(self.path_to_nonPreprocessedDataset)\n",
    "            X_train_dtm, _ = self.apply_gen_rules_features(df_notPreprocessed[\"tweet_text\"], X_train_WoR_dtm, tokenizer, \\\n",
    "                                                      X_train_Features)\n",
    "        else:\n",
    "            X_train_dtm = X_train_WoR_dtm\n",
    "        inv_tokenizer = self.invTokenizer(tokenizer)\n",
    "        fun_word_to_vec_map = self.load_word2vec_model(pathToWord2VecPretrained)\n",
    "        vector = []\n",
    "       \n",
    "        for tw_sequence in X_train_dtm:\n",
    "            vec = self.word_vector_avg(inv_tokenizer, fun_word_to_vec_map, tw_sequence, embed_dim)\n",
    "            vector.append(vec)\n",
    "        X_train_dtm = np.array(vector)\n",
    "        return X_train_dtm\n",
    "    \n",
    "    def load_word2vec_model(self, path):\n",
    "        if \"Model\" not in path:\n",
    "            print(\"You may not loading a model; You may loading a tokenizer.\")\n",
    "        if \"glove\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"word2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"google\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"doc2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "            \n",
    "        elif \"crisisNLP2vec\".lower() in path.lower().split(\"/\")[2]:\n",
    "             word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    def get_predictions(self, clf_model, X):\n",
    "        return clf_model.predict(X)\n",
    "    \n",
    "    def generateNameFromDataSetName(self, list1):\n",
    "        str1 = \"\"\n",
    "        for e in list1:\n",
    "            if e == \"\":\n",
    "                str1 += '.'.join(str(e))\n",
    "            else:\n",
    "                if e == \"csv\":\n",
    "                    e = \"_withPredictions.\" + e\n",
    "                    str1 += ''.join(str(e))\n",
    "                else:\n",
    "                    str1 += ''.join(str(e))\n",
    "        return \"..\" + str1\n",
    "\n",
    "    def generatePredWord2VecPretrained(self, path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                     pathToClfModel, path2word2vecModel, pathToWord2VecPretrained):\n",
    "        NotProcessedDataWithPredictions = pd.read_csv(obj.path_to_nonPreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        df = pd.read_csv(obj.path_to_PreprocessedDataset,  encoding = \"ISO-8859-1\")\n",
    "        for vect_filename in glob.glob(pathToVectFolder + \"/*\"):\n",
    "            print(40*\"+\")\n",
    "            obj.traditional_ChangeVectPath(vect_filename)\n",
    "            Vect = obj.traditional_loadVect()\n",
    "            for pretrainedWord2VecModel_filename in glob.glob(pathToWord2VecPretrained + \"*\"): \n",
    "                if pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\") == \\\n",
    "                vect_filename.split(\"/\")[-1].split(\"_\"):\n",
    "                    obj.word2vec_ChangeWord2VecPretrainedPath(pretrainedWord2VecModel_filename)\n",
    "                    for clfModel_filename in glob.glob(pathToClfFolder + \"*\"):\n",
    "                        if clfModel_filename.split(\"/\")[-1].split(\"_\")[6:-1] == pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\"):\n",
    "                            obj.traditional_ChangeClfModelPath(clfModel_filename)\n",
    "                            name_cell = obj.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:]\n",
    "                            if name_cell[-3] == \"WRul\":\n",
    "                                obj.include_rules = True\n",
    "                            else:\n",
    "                                obj.include_rules = False\n",
    "                            X = obj.word2vec_Vect(pretrainedWord2VecModel_filename, df)\n",
    "                            clf_model = obj.traditional_loadClf()\n",
    "                            y_pred = obj.get_predictions(clf_model, X)\n",
    "                            name_cell = name_cell[-6] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1]\n",
    "                            print(name_cell)\n",
    "                            NotProcessedDataWithPredictions[name_cell] = y_pred.tolist()\n",
    "        notProcessedDatasetName = obj.generateNameFromDataSetName(obj.path_to_nonPreprocessedDataset.split(\".\"))\n",
    "        NotProcessedDataWithPredictions.to_csv(notProcessedDatasetName, index=False)\n",
    "        print(40*\"-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4bcfedfb-a66b-4406-be1b-892192c6dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WRul_CountVect_Freq-2959_GB\n",
      "Bigrams_WRul_CountVect_Freq-2959_LR\n",
      "Bigrams_WRul_CountVect_Freq-2959_MLP\n",
      "Bigrams_WRul_CountVect_Freq-2959_NB\n",
      "Bigrams_WRul_CountVect_Freq-2959_RF\n",
      "Bigrams_WRul_CountVect_Freq-2959_SVM\n",
      "Bigrams_WRul_CountVect_Freq-2959_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_DT\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_GB\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_LR\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_MLP\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_NB\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_RF\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WRul_CountVect_Freq-3561_DT\n",
      "Trigrams_WRul_CountVect_Freq-3561_GB\n",
      "Trigrams_WRul_CountVect_Freq-3561_LR\n",
      "Trigrams_WRul_CountVect_Freq-3561_MLP\n",
      "Trigrams_WRul_CountVect_Freq-3561_NB\n",
      "Trigrams_WRul_CountVect_Freq-3561_RF\n",
      "Trigrams_WRul_CountVect_Freq-3561_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_GB\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_LR\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_MLP\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_NB\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_RF\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_SVM\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_GB\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_LR\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_MLP\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_NB\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_RF\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_SVM\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WRul_CountVect_Freq-1274_DT\n",
      "Unigrams_WRul_CountVect_Freq-1274_GB\n",
      "Unigrams_WRul_CountVect_Freq-1274_LR\n",
      "Unigrams_WRul_CountVect_Freq-1274_MLP\n",
      "Unigrams_WRul_CountVect_Freq-1274_NB\n",
      "Unigrams_WRul_CountVect_Freq-1274_RF\n",
      "Unigrams_WRul_CountVect_Freq-1274_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WoRul_CountVect_Freq-2941_GB\n",
      "Bigrams_WoRul_CountVect_Freq-2941_DT\n",
      "Bigrams_WoRul_CountVect_Freq-2941_LR\n",
      "Bigrams_WoRul_CountVect_Freq-2941_MLP\n",
      "Bigrams_WoRul_CountVect_Freq-2941_NB\n",
      "Bigrams_WoRul_CountVect_Freq-2941_RF\n",
      "Bigrams_WoRul_CountVect_Freq-2941_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_DT\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_GB\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_LR\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_MLP\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_NB\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_RF\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WoRul_CountVect_Freq-3543_DT\n",
      "Trigrams_WoRul_CountVect_Freq-3543_GB\n",
      "Trigrams_WoRul_CountVect_Freq-3543_LR\n",
      "Trigrams_WoRul_CountVect_Freq-3543_MLP\n",
      "Trigrams_WoRul_CountVect_Freq-3543_NB\n",
      "Trigrams_WoRul_CountVect_Freq-3543_RF\n",
      "Trigrams_WoRul_CountVect_Freq-3543_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_DT\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_GB\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_LR\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_MLP\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_NB\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_RF\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_DT\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_GB\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_LR\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_MLP\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_NB\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_RF\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WoRul_CountVect_Freq-1256_DT\n",
      "Unigrams_WoRul_CountVect_Freq-1256_GB\n",
      "Unigrams_WoRul_CountVect_Freq-1256_LR\n",
      "Unigrams_WoRul_CountVect_Freq-1256_MLP\n",
      "Unigrams_WoRul_CountVect_Freq-1256_NB\n",
      "Unigrams_WoRul_CountVect_Freq-1256_RF\n",
      "Unigrams_WoRul_CountVect_Freq-1256_SVM\n",
      "----------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_DT\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_GB\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_LR\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_MLP\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_NB\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_RF\n",
      "Bigrams_WoRul_TfIdfVect_Freq-2941_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_DT\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_GB\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_LR\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_MLP\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_NB\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_RF\n",
      "BiTrigrams_WoRul_TfIdfVect_Freq-6484_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_DT\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_GB\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_LR\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_MLP\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_NB\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_RF\n",
      "Trigrams_WoRul_TfIdfVect_Freq-3543_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_SVM\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_DT\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_GB\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_LR\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_MLP\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_NB\n",
      "UniAndBigrams_WoRul_TfIdfVect_Freq-4197_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_GB\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_LR\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_MLP\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_NB\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_RF\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_SVM\n",
      "UniBiAndTrigrams_WoRul_TfIdfVect_Freq-7740_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_DT\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_GB\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_LR\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_MLP\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_NB\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_RF\n",
      "Unigrams_WoRul_TfIdfVect_Freq-1256_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_GB\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_DT\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_LR\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_MLP\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_NB\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_RF\n",
      "Bigrams_WRul_TfIdfVect_Freq-2959_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_DT\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_GB\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_LR\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_MLP\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_NB\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_RF\n",
      "BiTrigrams_WRul_TfIdfVect_Freq-6502_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_DT\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_GB\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_LR\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_MLP\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_NB\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_RF\n",
      "Trigrams_WRul_TfIdfVect_Freq-3561_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_GB\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_LR\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_MLP\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_NB\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_RF\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_SVM\n",
      "UniAndBigrams_WRul_TfIdfVect_Freq-4215_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_DT\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_GB\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_LR\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_MLP\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_NB\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_RF\n",
      "UniBiAndTrigrams_WRul_TfIdfVect_Freq-7758_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_DT\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_GB\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_LR\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_MLP\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_NB\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_RF\n",
      "Unigrams_WRul_TfIdfVect_Freq-1274_SVM\n",
      "----------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_DT\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_GB\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_LR\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_MLP\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_NB\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_RF\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_SVM\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_DT\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_GB\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_LR\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_MLP\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_NB\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pathToClfModel = \"\" \n",
    "path2word2vecModel = 'dummyPath/word2vecModels/word2vec'\n",
    "pathToWord2VecPretrained = \"../word2vec_preTrainedModel/\"\n",
    "\n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "\n",
    "pathToVectFolder = \"../simple_vect/\"\n",
    "pathToClfFolder = \"../simple_clfModels/\"\n",
    "obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredNGramsPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)\n",
    "\n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample_withPredictions.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "\n",
    "pathToVectFolder = \"../tfidf_vect/\"\n",
    "pathToClfFolder = \"../tfidf_clfModels/\"\n",
    "obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredNGramsPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)\n",
    "\n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample_withPredictions_withPredictions.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "\n",
    "pathToVectFolder = \"../word2vec_vect/\"\n",
    "pathToClfFolder = \"../word2vec_clfModels/\"\n",
    "obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredWord2VecPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7affce25-ff88-4c21-9d8c-98ea866f231e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b2d67-2ffe-47f8-a065-1bcf9b345533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 238)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fffff0-81dd-486e-90f8-b6de44d389cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                  130\n",
       "0                  108\n",
       "2354                 1\n",
       "2.66e+17             1\n",
       "sandy hurricane      1\n",
       "request              1\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba734536-4d2d-442e-a0f0-e64fb3fd9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_from_rows_into_column(df, indexes):\n",
    "    \"\"\" The function takes dataframe and index of columns, and \n",
    "    then it returns the array containg the most frequent value against each row.\n",
    "    Example: \n",
    "    dataframe:\n",
    "    'c1' | 'c2'| 'c3' | 'c4'\n",
    "    -------------------------\n",
    "      1  |  1  |   1  |  0  \n",
    "      2  |  1  |   2  |  2\n",
    "      2  |  3  |   3  |  0 \n",
    "      2  |  0  |   0  |  0 \n",
    "      returned array:\n",
    "      (1, 2, 3, 0)\n",
    "      df: dataframe\n",
    "      indexes: indexes of columns\n",
    "      example: get_max_from_rows_into_column(dataframe, -3:)\n",
    "      \"\"\"\n",
    "    def get_array(df):\n",
    "        max_unique_label_column = np.empty([0])\n",
    "        for v in df.values:\n",
    "            unique_label, freq_label = np.unique(v, return_counts=True)\n",
    "            # print(unique_label[np.argmax(freq_label)])\n",
    "            max_unique_label_column = np.append(max_unique_label_column, unique_label[np.argmax(freq_label)])\n",
    "            \n",
    "        return max_unique_label_column\n",
    "    if \":\" in indexes:\n",
    "        if indexes[0] == \":\":\n",
    "            \n",
    "            df = df.iloc[:, :int(indexes[1:])]\n",
    "            max_unique_label_column = get_array(df)\n",
    "        if indexes[-1] == \":\":\n",
    "            df = df.iloc[:, int(indexes[:-1]):]\n",
    "            max_unique_label_column = get_array(df)\n",
    "           \n",
    "        else:\n",
    "            indexes = [int(x) for x in indexes.split(\":\")]\n",
    "            df = df.iloc[:, indexes[0]: indexes[1]]\n",
    "            max_unique_label_column = get_array(df)\n",
    "    else:\n",
    "            indexes = [int(x) for x in indexes.split(\",\")]\n",
    "            df = df.iloc[:, indexes]\n",
    "            max_unique_label_column = get_array(df)\n",
    "    return max_unique_label_column.astype(int)\n",
    "path = \"500_random_sample_withPredictions_withPredictions_withPredictions.csv\"\n",
    "df = pd.read_csv(path,  encoding = \"ISO-8859-1\")\n",
    "\n",
    "indexes = \"4:\"\n",
    "df[\"tweet_class\"] = get_max_from_rows_into_column(df, indexes)\n",
    "df = df[[\"tweetID\", \"tweet_text\", \"tweet_class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d1a73-0613-49b0-a357-1cc311d6a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 122\n",
      "True\n",
      "1 : 130\n",
      "True\n",
      "1 : 192\n",
      "True\n",
      "0 : 125\n",
      "True\n",
      "0 : 181\n",
      "True\n",
      "1 : 221\n",
      "True\n",
      "1 : 225\n",
      "True\n",
      "1 : 167\n",
      "True\n",
      "0 : 149\n",
      "True\n",
      "1 : 179\n",
      "True\n",
      "1 : 221\n",
      "True\n",
      "0 : 196\n",
      "True\n",
      "1 : 215\n",
      "True\n",
      "0 : 207\n",
      "True\n",
      "1 : 193\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "0 : 153\n",
      "True\n",
      "0 : 204\n",
      "True\n",
      "0 : 166\n",
      "True\n",
      "1 : 215\n",
      "True\n",
      "1 : 133\n",
      "True\n",
      "1 : 223\n",
      "True\n",
      "1 : 203\n",
      "True\n",
      "1 : 136\n",
      "True\n",
      "1 : 204\n",
      "True\n",
      "0 : 198\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "1 : 168\n",
      "True\n",
      "1 : 217\n",
      "True\n",
      "0 : 207\n",
      "True\n",
      "1 : 197\n",
      "True\n",
      "0 : 142\n",
      "True\n",
      "1 : 214\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "1 : 176\n",
      "True\n",
      "0 : 206\n",
      "True\n",
      "1 : 124\n",
      "True\n",
      "1 : 224\n",
      "True\n",
      "1 : 193\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "1 : 216\n",
      "True\n",
      "1 : 198\n",
      "True\n",
      "1 : 217\n",
      "True\n",
      "1 : 122\n",
      "True\n",
      "1 : 196\n",
      "True\n",
      "0 : 168\n",
      "True\n",
      "0 : 149\n",
      "True\n",
      "0 : 170\n",
      "True\n",
      "0 : 191\n",
      "True\n",
      "0 : 178\n",
      "True\n",
      "1 : 120\n",
      "True\n",
      "1 : 176\n",
      "True\n",
      "0 : 150\n",
      "True\n",
      "1 : 231\n",
      "True\n",
      "1 : 203\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "1 : 233\n",
      "True\n",
      "1 : 225\n",
      "True\n",
      "1 : 179\n",
      "True\n",
      "0 : 131\n",
      "True\n",
      "0 : 154\n",
      "True\n",
      "1 : 190\n",
      "True\n",
      "0 : 171\n",
      "True\n",
      "0 : 193\n",
      "True\n",
      "0 : 194\n",
      "True\n",
      "1 : 160\n",
      "True\n",
      "0 : 174\n",
      "True\n",
      "1 : 207\n",
      "True\n",
      "0 : 206\n",
      "True\n",
      "0 : 204\n",
      "True\n",
      "1 : 221\n",
      "True\n",
      "0 : 144\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "0 : 158\n",
      "True\n",
      "1 : 209\n",
      "True\n",
      "1 : 213\n",
      "True\n",
      "1 : 127\n",
      "True\n",
      "1 : 203\n",
      "True\n",
      "1 : 141\n",
      "True\n",
      "0 : 131\n",
      "True\n",
      "0 : 145\n",
      "True\n",
      "0 : 207\n",
      "True\n",
      "1 : 228\n",
      "True\n",
      "0 : 149\n",
      "True\n",
      "1 : 214\n",
      "True\n",
      "1 : 231\n",
      "True\n",
      "0 : 197\n",
      "True\n",
      "0 : 208\n",
      "True\n",
      "0 : 157\n",
      "True\n",
      "1 : 204\n",
      "True\n",
      "0 : 145\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "0 : 193\n",
      "True\n",
      "0 : 175\n",
      "True\n",
      "1 : 144\n",
      "True\n",
      "0 : 174\n",
      "True\n",
      "1 : 199\n",
      "True\n",
      "1 : 184\n",
      "True\n",
      "0 : 202\n",
      "True\n",
      "0 : 201\n",
      "True\n",
      "1 : 169\n",
      "True\n",
      "1 : 227\n",
      "True\n",
      "1 : 160\n",
      "True\n",
      "0 : 185\n",
      "True\n",
      "0 : 161\n",
      "True\n",
      "1 : 208\n",
      "True\n",
      "0 : 201\n",
      "True\n",
      "0 : 169\n",
      "True\n",
      "1 : 206\n",
      "True\n",
      "1 : 153\n",
      "True\n",
      "0 : 182\n",
      "True\n",
      "0 : 196\n",
      "True\n",
      "0 : 181\n",
      "True\n",
      "0 : 207\n",
      "True\n",
      "0 : 208\n",
      "True\n",
      "0 : 193\n",
      "True\n",
      "1 : 206\n",
      "True\n",
      "1 : 138\n",
      "True\n",
      "1 : 204\n",
      "True\n",
      "0 : 184\n",
      "True\n",
      "1 : 169\n",
      "True\n",
      "0 : 165\n",
      "True\n",
      "0 : 153\n",
      "True\n",
      "1 : 189\n",
      "True\n",
      "0 : 204\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "0 : 181\n",
      "True\n",
      "0 : 189\n",
      "True\n",
      "0 : 148\n",
      "True\n",
      "1 : 215\n",
      "True\n",
      "1 : 214\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "1 : 206\n",
      "True\n",
      "1 : 220\n",
      "True\n",
      "0 : 167\n",
      "True\n",
      "1 : 196\n",
      "True\n",
      "1 : 214\n",
      "True\n",
      "0 : 162\n",
      "True\n",
      "0 : 152\n",
      "True\n",
      "1 : 194\n",
      "True\n",
      "0 : 156\n",
      "True\n",
      "1 : 187\n",
      "True\n",
      "1 : 136\n",
      "True\n",
      "1 : 223\n",
      "True\n",
      "1 : 209\n",
      "True\n",
      "1 : 231\n",
      "True\n",
      "1 : 186\n",
      "True\n",
      "1 : 230\n",
      "True\n",
      "0 : 206\n",
      "True\n",
      "1 : 230\n",
      "True\n",
      "1 : 203\n",
      "True\n",
      "0 : 132\n",
      "True\n",
      "1 : 205\n",
      "True\n",
      "0 : 205\n",
      "True\n",
      "1 : 153\n",
      "True\n",
      "1 : 201\n",
      "True\n",
      "1 : 208\n",
      "True\n",
      "0 : 156\n",
      "True\n",
      "0 : 158\n",
      "True\n",
      "1 : 193\n",
      "True\n",
      "0 : 170\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "0 : 129\n",
      "True\n",
      "1 : 227\n",
      "True\n",
      "1 : 212\n",
      "True\n",
      "1 : 197\n",
      "True\n",
      "1 : 224\n",
      "True\n",
      "1 : 201\n",
      "True\n",
      "0 : 172\n",
      "True\n",
      "0 : 169\n",
      "True\n",
      "0 : 150\n",
      "True\n",
      "0 : 157\n",
      "True\n",
      "0 : 192\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "1 : 148\n",
      "True\n",
      "1 : 193\n",
      "True\n",
      "0 : 196\n",
      "True\n",
      "0 : 194\n",
      "True\n",
      "0 : 189\n",
      "True\n",
      "0 : 168\n",
      "True\n",
      "0 : 142\n",
      "True\n",
      "1 : 219\n",
      "True\n",
      "0 : 162\n",
      "True\n",
      "0 : 187\n",
      "True\n",
      "1 : 203\n",
      "True\n",
      "0 : 178\n",
      "True\n",
      "1 : 194\n",
      "True\n",
      "0 : 124\n",
      "True\n",
      "0 : 194\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "0 : 150\n",
      "True\n",
      "1 : 208\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "1 : 233\n",
      "True\n",
      "0 : 169\n",
      "True\n",
      "0 : 181\n",
      "True\n",
      "0 : 199\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "0 : 175\n",
      "True\n",
      "1 : 221\n",
      "True\n",
      "0 : 200\n",
      "True\n",
      "0 : 191\n",
      "True\n",
      "0 : 193\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "1 : 182\n",
      "True\n",
      "1 : 173\n",
      "True\n",
      "0 : 204\n",
      "True\n",
      "0 : 137\n",
      "True\n",
      "1 : 198\n",
      "True\n",
      "0 : 193\n",
      "True\n",
      "1 : 206\n",
      "True\n",
      "0 : 150\n",
      "True\n",
      "0 : 196\n",
      "True\n",
      "0 : 200\n",
      "True\n",
      "1 : 226\n",
      "True\n",
      "0 : 164\n",
      "True\n",
      "1 : 229\n",
      "True\n",
      "1 : 227\n",
      "True\n",
      "1 : 216\n",
      "True\n",
      "0 : 163\n",
      "True\n",
      "1 : 211\n",
      "True\n",
      "1 : 136\n",
      "True\n",
      "1 : 183\n",
      "True\n",
      "1 : 208\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "0 : 163\n",
      "True\n",
      "0 : 189\n",
      "True\n",
      "1 : 218\n",
      "True\n",
      "0 : 165\n",
      "True\n",
      "0 : 207\n",
      "True\n",
      "1 : 220\n",
      "True\n",
      "1 : 155\n",
      "True\n",
      "1 : 228\n",
      "True\n",
      "0 : 167\n",
      "True\n",
      "1 : 216\n",
      "True\n",
      "0 : 176\n",
      "True\n",
      "1 : 194\n",
      "True\n",
      "0 : 192\n",
      "True\n",
      "0 : 194\n",
      "True\n",
      "1 : 232\n",
      "True\n",
      "1 : 225\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "path = \"500_random_sample_withPredictions_withPredictions_withPredictions.csv\"\n",
    "dff = pd.read_csv(path,  encoding = \"ISO-8859-1\")\n",
    "i = 01\n",
    "c = dff.iloc[i, :]\n",
    "\n",
    "if c.value_counts()[0] > c.value_counts()[1]:\n",
    "    print(0, \":\", c.value_counts()[0])ounts()[0]:\n",
    "    print(1, \":\", c.value_counts()[1])\n",
    "    rint(df[\"tweet_class\"][i] == 1)\n",
    "df[\"tweet_class\"][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "659b401d-fd53-4de2-a796-10f2addc4445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 122)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e5584-07aa-4927-bbdc-3c89edc4498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[22-05-06_22-56-41_Vect_WoRul_Max-Len-Seq-26_Freq-300_22-05-06_22-56-41_word2vec_Vect_WoRul_Max-Len-Seq-26_Freq-300]\n",
    "==\n",
    "['22-05-06', '22-56-41', 'word2vec', 'Model', 'WoRul', 'Max-Len-Seq-26', 'Freq-300']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virEnv375",
   "language": "python",
   "name": "virenv375"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

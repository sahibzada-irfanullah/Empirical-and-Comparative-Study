{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6556751-28e8-47b9-81b5-6f5cff23e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 22:50:04.754975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import scipy\n",
    "import numpy as np\n",
    "import glob\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6204a69-649d-4341-8149-a9da332f5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsePretrainedVectClf:\n",
    "    def __init__(self, pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "        self.path_to_PreprocessedDataset = path_to_PreprocessedDataset\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "    \n",
    "    def traditional_ChangeVectPath(self, pathToTokenizer):\n",
    "        self.pathToTokenizer = pathToTokenizer   \n",
    "        \n",
    "    def traditional_ChangeClfModelPath(self, pathToClfModel):\n",
    "        self.pathToClfModel = pathToClfModel     \n",
    "        \n",
    "    def traditional_loadVect(self):\n",
    "        Vect = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        return Vect\n",
    "    \n",
    "    def apply_gen_rules_features_ngrams(self, X, X_train_WoR_dtm, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_ngrams(X)\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_features\n",
    "\n",
    "\n",
    "    def gen_rules_features_ngrams(self, X_data_series):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "        it as features...\n",
    "        I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "          for changing datatypes'''\n",
    "        \n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            columnName = \"RegEx_\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_ngrams(text, regex))\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "        return temp_sparse, features_arg\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def apply_regex_ngrams(self, text, regex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        match_found = int(match_found == True)\n",
    "        return match_found\n",
    "    \n",
    "    def traditional_getFeatures(self, Vect):\n",
    "        df = pd.read_csv(self.path_to_PreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        X_train_WoR_dtm = Vect.transform(df[\"tweet_text\"])\n",
    "        X_train_Features = list(Vect.get_feature_names_out())\n",
    "        if self.include_rules:\n",
    "            df_notPreprocessed = pd.read_csv(self.path_to_nonPreprocessedDataset,  encoding = \"ISO-8859-1\")\n",
    "            X_train_dtm, X_train_Features = self.apply_gen_rules_features_ngrams(df_notPreprocessed[\"tweet_text\"], \\\n",
    "                                                                      X_train_WoR_dtm, X_train_Features)\n",
    "        else:\n",
    "            X_train_dtm = X_train_WoR_dtm\n",
    "        return X_train_dtm.toarray(), Vect\n",
    "\n",
    "    def traditional_loadClf(self):  \n",
    "        clf_model = pickle.load(open(self.pathToClfModel, 'rb'))\n",
    "        return clf_model\n",
    "    \n",
    "    def get_predictions(self, clf_model, X):\n",
    "        return clf_model.predict(X)\n",
    "    \n",
    "    def generateNameFromDataSetName(self, list1):\n",
    "        str1 = \"\"\n",
    "        for e in list1:\n",
    "            if e == \"\":\n",
    "                str1 += '.'.join(str(e))\n",
    "            else:\n",
    "                if e == \"csv\":\n",
    "                    e = \"_withPredictions.\" + e\n",
    "                    str1 += ''.join(str(e))\n",
    "                else:\n",
    "                    str1 += ''.join(str(e))\n",
    "        return \"..\" + str1        \n",
    "    \n",
    "    def evaluation(self):\n",
    "        if \"WRul\" in self.pathToTokenizer:\n",
    "            if not self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WoRul\" in self.pathToTokenizer:\n",
    "            if self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WRul\" in self.pathToClfModel:\n",
    "            if not self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if \"WoRul\" in self.pathToClfModel:\n",
    "            if self.include_rules:\n",
    "                print(\"Vectoirzer and appending does not match\")\n",
    "        if not self.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:-1] == self.pathToTokenizer.split(\"/\")[-1].split(\"_\"):\n",
    "            print(\"Vectorizer and classifer does not match.\")\n",
    "            print(\"Vectorizer:\", self.pathToTokenizer)\n",
    "            print(\"Classifier:\", self.pathToClfModel)\n",
    "            print(\"Classifier:\", self.pathToTokenizer.split(\"/\")[-1].split(\"_\"), \"Vectorizer:\" , self.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:-1])\n",
    "\n",
    "    def generatePredNGramsPretrained(self, path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                         pathToClfModel, path2word2vecModel, pathToWord2VecPretrained):\n",
    "\n",
    "        NotProcessedDataWithPredictions = pd.read_csv(obj.path_to_nonPreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        for vect_filename in glob.glob(pathToVectFolder + \"/*\"):\n",
    "            print(40*\"+\")\n",
    "            obj.traditional_ChangeVectPath(vect_filename)\n",
    "            Vect = obj.traditional_loadVect()\n",
    "            for clfModel_filename in glob.glob(pathToClfFolder + \"*\"):\n",
    "                if clfModel_filename.split(\"/\")[-1].split(\"_\")[5:-1] == vect_filename.split(\"/\")[-1].split(\"_\"):\n",
    "                    obj.traditional_ChangeClfModelPath(clfModel_filename)\n",
    "                    name_cell = obj.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:]\n",
    "                    if name_cell[-3] == \"WRul\":\n",
    "                        obj.include_rules = True\n",
    "                    else:\n",
    "                        obj.include_rules = False\n",
    "                    # print(obj.pathToTokenizer, obj.pathToClfModel)\n",
    "                    # print(name_cell)\n",
    "                    # print(name_cell[-5] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1])\n",
    "                    X, _ = obj.traditional_getFeatures(Vect)\n",
    "                    clf_model = obj.traditional_loadClf()\n",
    "                    y_pred = obj.get_predictions(clf_model, X)\n",
    "                    name_cell = name_cell[-5] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1]\n",
    "                    print(name_cell)\n",
    "                    NotProcessedDataWithPredictions[name_cell] = y_pred.tolist()\n",
    "        notProcessedDatasetName = obj.generateNameFromDataSetName(obj.path_to_nonPreprocessedDataset.split(\".\"))\n",
    "        NotProcessedDataWithPredictions.to_csv(notProcessedDatasetName)\n",
    "        print(40*\"-\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887056e3-f37b-40b7-9f78-735f1e3b50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "word2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "doc2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_DT\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_GB\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_LR\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_MLP\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_NB\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_RF\n",
      "glove_Max-Len-Seq-20_WoRul_Freq-200_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "google2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_SVM\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_DT\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_GB\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_LR\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_MLP\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_NB\n",
      "crisisNLP2vec_Max-Len-Seq-20_WoRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "word2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "doc2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_SVM\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_DT\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_GB\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_LR\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_MLP\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_NB\n",
      "glove_Max-Len-Seq-38_WRul_Freq-200_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "google2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_DT\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_GB\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_LR\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_MLP\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_NB\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_RF\n",
      "crisisNLP2vec_Max-Len-Seq-38_WRul_Freq-300_SVM\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Use_Word2Vec:\n",
    "    def __init__(self,pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "        self.path_to_PreprocessedDataset = path_to_PreprocessedDataset\n",
    "\n",
    "    \n",
    "    def word_vector_avg(self, inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "        vec = np.zeros(size)\n",
    "        count = 0\n",
    "        columnNameList = []\n",
    "        for i in range(18):\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            columnNameList.append(columnName)\n",
    "        for seq in tw_sequence:\n",
    "            try:\n",
    "                if inv_tokenizer[seq] in columnNameList:\n",
    "                    embedding_vector = np.ndarray(shape=(size,))\n",
    "                    embedding_vector[:] = 1/size\n",
    "                else:\n",
    "                    embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "                if embedding_vector is not None:\n",
    "                    vec += embedding_vector\n",
    "                else:\n",
    "                    vec += np.zeros(size)\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def apply_regex_word2vec(self, text, regex, ruleTokenizerSequenceIndex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        if match_found:\n",
    "            match_found = int(ruleTokenizerSequenceIndex)\n",
    "        else:\n",
    "            match_found = 0\n",
    "        return match_found\n",
    "    \n",
    "    def gen_rules_features_word2vec(self, X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            # we can also use ruleString()\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_word2vec(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "\n",
    "        return temp_sparse, features_arg\n",
    "    def apply_gen_rules_features_word2vec(self, X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_word2vec(X, tokenizer)\n",
    "\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_feature\n",
    "\n",
    "    \n",
    "    def traditional_ChangeVectPath(self, pathToTokenizer):\n",
    "        self.pathToTokenizer = pathToTokenizer \n",
    "        \n",
    "    def word2vec_ChangeWord2VecPretrainedPath(self, pathToWord2VecPretrained):\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained \n",
    "\n",
    "    def word2vec_loadWord2VecPretrained(self):  \n",
    "        word2vec_model = pickle.load(open(self.pathToWord2VecPretrained, 'rb'))\n",
    "        return word2vec_model\n",
    "        \n",
    "    def traditional_loadVect(self):\n",
    "        Vect = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        return Vect\n",
    "    \n",
    "    def traditional_loadClf(self):  \n",
    "        clf_model = pickle.load(open(self.pathToClfModel, 'rb'))\n",
    "        return clf_model\n",
    "\n",
    "    def traditional_ChangeClfModelPath(self, pathToClfModel):\n",
    "        self.pathToClfModel = pathToClfModel  \n",
    "\n",
    "    def invTokenizer(self, tokenizer):\n",
    "        inv_tokenizer = {v: k for k, v in tokenizer.word_index.items()}\n",
    "        return inv_tokenizer\n",
    "\n",
    "    def word2vec_Vect(self, pathToWord2VecPretrained, df):\n",
    "        tokenizer = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        embed_dim = int(self.pathToTokenizer.split(\"Freq-\")[1])\n",
    "        if \"WRul\" in self.pathToTokenizer:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0]) - 18\n",
    "        else:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0])\n",
    "\n",
    "        sequences = tokenizer.texts_to_sequences(df['tweet_text'])\n",
    "        X_train_WoR_dtm = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        X_train_Features = [\"_Not_Apply_\"]\n",
    "        # print(type(X_train_WoR_dtm), X_train_WoR_dtm.shape)\n",
    "        if self.include_rules:\n",
    "            df_notPreprocessed = pd.read_csv(self.path_to_nonPreprocessedDataset)\n",
    "            X_train_dtm, _ = self.apply_gen_rules_features(df_notPreprocessed[\"tweet_text\"], X_train_WoR_dtm, tokenizer, \\\n",
    "                                                      X_train_Features)\n",
    "        else:\n",
    "            X_train_dtm = X_train_WoR_dtm\n",
    "        inv_tokenizer = self.invTokenizer(tokenizer)\n",
    "        fun_word_to_vec_map = self.load_word2vec_model(pathToWord2VecPretrained)\n",
    "        vector = []\n",
    "        for tw_sequence in X_train_dtm:\n",
    "            vec = self.word_vector_avg(inv_tokenizer, fun_word_to_vec_map, tw_sequence, embed_dim)\n",
    "            vector.append(vec)\n",
    "        X_train_dtm = np.array(vector)\n",
    "        return X_train_dtm\n",
    "    \n",
    "    def load_word2vec_model(self, path):\n",
    "        if \"Model\" not in path:\n",
    "            print(\"You may not loading a model; You may loading a tokenizer.\")\n",
    "        if \"glove\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"word2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"google\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"doc2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "            \n",
    "        elif \"crisisNLP2vec\".lower() in path.lower().split(\"/\")[2]:\n",
    "             word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    def get_predictions(self, clf_model, X):\n",
    "        return clf_model.predict(X)\n",
    "    \n",
    "    def generateNameFromDataSetName(self, list1):\n",
    "        str1 = \"\"\n",
    "        for e in list1:\n",
    "            if e == \"\":\n",
    "                str1 += '.'.join(str(e))\n",
    "            else:\n",
    "                if e == \"csv\":\n",
    "                    e = \"_withPredictions.\" + e\n",
    "                    str1 += ''.join(str(e))\n",
    "                else:\n",
    "                    str1 += ''.join(str(e))\n",
    "        return \"..\" + str1\n",
    "\n",
    "    def generatePredWord2VecPretrained(self, path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                     pathToClfModel, path2word2vecModel, pathToWord2VecPretrained):\n",
    "        NotProcessedDataWithPredictions = pd.read_csv(obj.path_to_nonPreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        df = pd.read_csv(obj.path_to_PreprocessedDataset,  encoding = \"ISO-8859-1\")\n",
    "        for vect_filename in glob.glob(pathToVectFolder + \"/*\"):\n",
    "            print(40*\"+\")\n",
    "            obj.traditional_ChangeVectPath(vect_filename)\n",
    "            Vect = obj.traditional_loadVect()\n",
    "            for pretrainedWord2VecModel_filename in glob.glob(pathToWord2VecPretrained + \"*\"): \n",
    "                if pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\") == \\\n",
    "                vect_filename.split(\"/\")[-1].split(\"_\"):\n",
    "                    obj.word2vec_ChangeWord2VecPretrainedPath(pretrainedWord2VecModel_filename)\n",
    "                    for clfModel_filename in glob.glob(pathToClfFolder + \"*\"):\n",
    "                        if clfModel_filename.split(\"/\")[-1].split(\"_\")[6:-1] == pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\"):\n",
    "                            obj.traditional_ChangeClfModelPath(clfModel_filename)\n",
    "                            name_cell = obj.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:]\n",
    "                            if name_cell[-3] == \"WRul\":\n",
    "                                obj.include_rules = True\n",
    "                            else:\n",
    "                                obj.include_rules = False\n",
    "                            X = obj.word2vec_Vect(pretrainedWord2VecModel_filename, df)\n",
    "                            clf_model = obj.traditional_loadClf()\n",
    "                            y_pred = obj.get_predictions(clf_model, X)\n",
    "                            name_cell = name_cell[-6] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1]\n",
    "                            print(name_cell)\n",
    "                            NotProcessedDataWithPredictions[name_cell] = y_pred.tolist()\n",
    "        notProcessedDatasetName = obj.generateNameFromDataSetName(obj.path_to_nonPreprocessedDataset.split(\".\"))\n",
    "        NotProcessedDataWithPredictions.to_csv(notProcessedDatasetName)\n",
    "        print(40*\"-\")\n",
    "    \n",
    "\n",
    "pathToClfModel = \"\" \n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "pathToVectFolder = \"../word2vec_vect/\"\n",
    "pathToClfFolder = \"../word2vec_clfModels/\"\n",
    "\n",
    "\n",
    "path2word2vecModel = 'dummyPath/word2vecModels/word2vec'\n",
    "pathToWord2VecPretrained = \"../word2vec_preTrainedModel/\"\n",
    "obj = Use_Word2Vec(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredWord2VecPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7eae2f1-2d87-47c7-bbc9-13c95aaf7c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WRul_CountVect_Freq-2959_GB\n",
      "Bigrams_WRul_CountVect_Freq-2959_LR\n",
      "Bigrams_WRul_CountVect_Freq-2959_MLP\n",
      "Bigrams_WRul_CountVect_Freq-2959_NB\n",
      "Bigrams_WRul_CountVect_Freq-2959_RF\n",
      "Bigrams_WRul_CountVect_Freq-2959_SVM\n",
      "Bigrams_WRul_CountVect_Freq-2959_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_DT\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_GB\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_LR\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_MLP\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_NB\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_RF\n",
      "BiTrigrams_WRul_CountVect_Freq-6502_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WRul_CountVect_Freq-3561_DT\n",
      "Trigrams_WRul_CountVect_Freq-3561_GB\n",
      "Trigrams_WRul_CountVect_Freq-3561_LR\n",
      "Trigrams_WRul_CountVect_Freq-3561_MLP\n",
      "Trigrams_WRul_CountVect_Freq-3561_NB\n",
      "Trigrams_WRul_CountVect_Freq-3561_RF\n",
      "Trigrams_WRul_CountVect_Freq-3561_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_GB\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_LR\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_MLP\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_NB\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_RF\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_SVM\n",
      "UniAndBigrams_WRul_CountVect_Freq-4215_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_GB\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_LR\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_MLP\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_NB\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_RF\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_SVM\n",
      "UniBiAndTrigrams_WRul_CountVect_Freq-7758_DT\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WRul_CountVect_Freq-1274_DT\n",
      "Unigrams_WRul_CountVect_Freq-1274_GB\n",
      "Unigrams_WRul_CountVect_Freq-1274_LR\n",
      "Unigrams_WRul_CountVect_Freq-1274_MLP\n",
      "Unigrams_WRul_CountVect_Freq-1274_NB\n",
      "Unigrams_WRul_CountVect_Freq-1274_RF\n",
      "Unigrams_WRul_CountVect_Freq-1274_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Bigrams_WoRul_CountVect_Freq-2941_GB\n",
      "Bigrams_WoRul_CountVect_Freq-2941_DT\n",
      "Bigrams_WoRul_CountVect_Freq-2941_LR\n",
      "Bigrams_WoRul_CountVect_Freq-2941_MLP\n",
      "Bigrams_WoRul_CountVect_Freq-2941_NB\n",
      "Bigrams_WoRul_CountVect_Freq-2941_RF\n",
      "Bigrams_WoRul_CountVect_Freq-2941_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_DT\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_GB\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_LR\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_MLP\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_NB\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_RF\n",
      "BiTrigrams_WoRul_CountVect_Freq-6484_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Trigrams_WoRul_CountVect_Freq-3543_DT\n",
      "Trigrams_WoRul_CountVect_Freq-3543_GB\n",
      "Trigrams_WoRul_CountVect_Freq-3543_LR\n",
      "Trigrams_WoRul_CountVect_Freq-3543_MLP\n",
      "Trigrams_WoRul_CountVect_Freq-3543_NB\n",
      "Trigrams_WoRul_CountVect_Freq-3543_RF\n",
      "Trigrams_WoRul_CountVect_Freq-3543_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_DT\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_GB\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_LR\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_MLP\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_NB\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_RF\n",
      "UniAndBigrams_WoRul_CountVect_Freq-4197_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_DT\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_GB\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_LR\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_MLP\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_NB\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_RF\n",
      "UniBiAndTrigrams_WoRul_CountVect_Freq-7740_SVM\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Unigrams_WoRul_CountVect_Freq-1256_DT\n",
      "Unigrams_WoRul_CountVect_Freq-1256_GB\n",
      "Unigrams_WoRul_CountVect_Freq-1256_LR\n",
      "Unigrams_WoRul_CountVect_Freq-1256_MLP\n",
      "Unigrams_WoRul_CountVect_Freq-1256_NB\n",
      "Unigrams_WoRul_CountVect_Freq-1256_RF\n",
      "Unigrams_WoRul_CountVect_Freq-1256_SVM\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pathToClfModel = \"\" \n",
    "path2word2vecModel = 'dummyPath/word2vecModels/word2vec'\n",
    "pathToWord2VecPretrained = \"../word2vec_preTrainedModel/\"\n",
    "\n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "pathToVectFolder = \"../simple_vect/\"\n",
    "pathToClfFolder = \"../simple_clfModels/\"\n",
    "obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "# obj.generatePredNGramsPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "#                                  pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)\n",
    "\n",
    "# path_to_nonPreprocessedDataset = \"../datasets/500_random_sample_withPredictions.csv\"\n",
    "# path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "# pathToVectFolder = \"../tfidf_vect/\"\n",
    "# pathToClfFolder = \"../tfidf_clfModels//\"\n",
    "# obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "#                    path_to_nonPreprocessedDataset)\n",
    "# obj.generatePredNGramsPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "#                                  pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a98707-5df3-4c7e-9aa7-a852f9779859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToClfModel = \"\" \n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "pathToVectFolder = \"../simple_vect/\"\n",
    "pathToClfFolder = \"../simple_clfModels/\"\n",
    "obj = UsePretrainedVectClf(pathToVectFolder, pathToClfModel, path_to_PreprocessedDataset, path_to_nonPreprocessedDataset, include_rules = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fffff0-81dd-486e-90f8-b6de44d389cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Use_Word2Vec:\n",
    "    def __init__(self,pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "        self.path_to_PreprocessedDataset = path_to_PreprocessedDataset\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "pathToClfModel = \"\" \n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "pathToVectFolder = \"../word2vec_vect/\"\n",
    "pathToClfFolder = \"../word2vec_clfModels/\"\n",
    "\n",
    "\n",
    "path2word2vecModel = 'dummyPath/word2vecModels/word2vec'\n",
    "pathToWord2VecPretrained = \"../word2vec_preTrainedModel/\"\n",
    "obj = Use_Word2Vec(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredWord2VecPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa5dbc6-7383-4aca-8d7e-b0aabc6f7557",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1119743166.py, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_14932/1119743166.py\"\u001b[0;36m, line \u001b[0;32m61\u001b[0m\n\u001b[0;31m    re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "class Use_Word2Vec:\n",
    "    def __init__(self,pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "        self.path_to_PreprocessedDataset = path_to_PreprocessedDataset    def __init__(self,pathToTokenizer, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                 path_to_nonPreprocessedDataset, include_rules = False):\n",
    "        self.pathToTokenizer = pathToTokenizer\n",
    "        self.pathToClfModel = pathToClfModel\n",
    "        self.include_rules = include_rules\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained\n",
    "        self.path_to_nonPreprocessedDataset = path_to_nonPreprocessedDataset\n",
    "    \n",
    "    def word_vector_avg(self, inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "        vec = np.zeros(size)\n",
    "        count = 0\n",
    "        columnNameList = []\n",
    "        for i in range(18):\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            columnNameList.append(columnName)\n",
    "        for seq in tw_sequence:\n",
    "            try:\n",
    "                if inv_tokenizer[seq] in columnNameList:\n",
    "                    embedding_vector = np.ndarray(shape=(size,))\n",
    "                    embedding_vector[:] = 1/size\n",
    "                else:\n",
    "                    embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "                if embedding_vector is not None:\n",
    "                    vec += embedding_vector\n",
    "                else:\n",
    "                    vec += np.zeros(size)\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def apply_regex_word2vec(self, text, regex, ruleTokenizerSequenceIndex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        if match_found:\n",
    "            match_found = int(ruleTokenizerSequenceIndex)\n",
    "        else:\n",
    "            match_found = 0\n",
    "        return match_found\n",
    "    \n",
    "    def gen_rules_features_word2vec(self, X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\    \n",
    "    def word_vector_avg(self, inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "        vec = np.zeros(size)\n",
    "        count = 0\n",
    "        columnNameList = []    \n",
    "    def word_vector_avg(self, inv_tokenizer, fun_word_to_vec_map, tw_sequence, size):\n",
    "        vec = np.zeros(size)\n",
    "        count = 0\n",
    "        columnNameList = []\n",
    "        for i in range(18):\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            columnNameList.append(columnName)\n",
    "        for seq in tw_sequence:\n",
    "            try:\n",
    "                if inv_tokenizer[seq] in columnNameList:\n",
    "                    embedding_vector = np.ndarray(shape=(size,))\n",
    "                    embedding_vector[:] = 1/size\n",
    "                else:\n",
    "                    embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "                if embedding_vector is not None:\n",
    "                    vec += embedding_vector\n",
    "                else:\n",
    "                    vec += np.zeros(size)\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def apply_regex_word2vec(self, text, regex, ruleTokenizerSequenceIndex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        if match_found:\n",
    "            match_found = int(ruleTokenizerSequenceIndex)\n",
    "        else:\n",
    "            match_found = 0\n",
    "        return match_found\n",
    "    \n",
    "    def gen_rules_features_word2vec(self, X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            # we can also use ruleString()\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_word2vec(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "\n",
    "        return temp_sparse, features_arg\n",
    "    def apply_gen_rules_features_word2vec(self, X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_word2vec(X, tokenizer)\n",
    "\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_feature\n",
    "\n",
    "        for i in range(18):\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            columnNameList.append(columnName)\n",
    "        for seq in tw_sequence:\n",
    "            try:\n",
    "                if inv_tokenizer[seq] in columnNameList:\n",
    "                    embedding_vector = np.ndarray(shape=(size,))\n",
    "                    embedding_vector[:] = 1/size\n",
    "                else:\n",
    "                    embedding_vector = fun_word_to_vec_map[inv_tokenizer[seq]]\n",
    "                if embedding_vector is not None:\n",
    "                    vec += embedding_vector\n",
    "                else:\n",
    "                    vec += np.zeros(size)\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    \n",
    "    def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "        combined_features = features_X + features_Rules\n",
    "        concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "        return concat_sparse, combined_features\n",
    "    \n",
    "    def series_to_DataFrame(self, X_data):\n",
    "        X_data = X_data.to_frame()\n",
    "        return X_data\n",
    "    \n",
    "    def apply_regex_word2vec(self, text, regex, ruleTokenizerSequenceIndex):\n",
    "        match_found = (re.search(regex, text) != None)\n",
    "        if match_found:\n",
    "            match_found = int(ruleTokenizerSequenceIndex)\n",
    "        else:\n",
    "            match_found = 0\n",
    "        return match_found\n",
    "    \n",
    "    def gen_rules_features_word2vec(self, X_data_series, tokenizer):  # , X_data_dtm, features_arg):\n",
    "        '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "        X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "        regexes = [\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            # we can also use ruleString()\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_word2vec(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "\n",
    "        return temp_sparse, features_arg\n",
    "    def apply_gen_rules_features_word2vec(self, X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_word2vec(X, tokenizer)\n",
    "\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_feature\n",
    "b(bringing|giving|helping|raising|donating|auctioning)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I\\'m|Im)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'re|we are)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b',\n",
    "                       re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(we\\'ll|we will)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b\\w*\\s*\\b\\?', re.I | re.M),\n",
    "            re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I | re.M),\n",
    "            re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I | re.M)\n",
    "\n",
    "        ]\n",
    "        temp = pd.DataFrame()\n",
    "        features_arg = []\n",
    "        for i, regex in zip(range(len(regexes)), regexes):\n",
    "            # we can also use ruleString()\n",
    "            columnName = \"rule\" + str(i + 1)\n",
    "            features_arg.append(columnName)\n",
    "            temp[columnName] = X_data_DF['tweet_text'].apply(lambda text: self.apply_regex_word2vec(text, regex, tokenizer.word_index[columnName]))\n",
    "\n",
    "        temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "\n",
    "        return temp_sparse, features_arg\n",
    "    def apply_gen_rules_features_word2vec(self, X, X_train_WoR_dtm, tokenizer, X_train_WoR_Features):\n",
    "        X_Rules_dtm, features_Rules = self.gen_rules_features_word2vec(X, tokenizer)\n",
    "\n",
    "        X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm,\n",
    "                                                                          X_train_WoR_Features, features_Rules)\n",
    "        return X_train_WR_dtm, combined_feature\n",
    "\n",
    "    \n",
    "    def traditional_ChangeVectPath(self, pathToTokenizer):\n",
    "        self.pathToTokenizer = pathToTokenizer \n",
    "        \n",
    "    def word2vec_ChangeWord2VecPretrainedPath(self, pathToWord2VecPretrained):\n",
    "        self.pathToWord2VecPretrained = pathToWord2VecPretrained \n",
    "\n",
    "    def word2vec_loadWord2VecPretrained(self):  \n",
    "        word2vec_model = pickle.load(open(self.pathToWord2VecPretrained, 'rb'))\n",
    "        return word2vec_model\n",
    "        \n",
    "    def traditional_loadVect(self):\n",
    "        Vect = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        return Vect\n",
    "    \n",
    "    def traditional_loadClf(self):  \n",
    "        clf_model = pickle.load(open(self.pathToClfModel, 'rb'))\n",
    "        return clf_model\n",
    "\n",
    "    def traditional_ChangeClfModelPath(self, pathToClfModel):\n",
    "        self.pathToClfModel = pathToClfModel  \n",
    "\n",
    "    def invTokenizer(self, tokenizer):\n",
    "        inv_tokenizer = {v: k for k, v in tokenizer.word_index.items()}\n",
    "        return inv_tokenizer\n",
    "\n",
    "    def word2vec_Vect(self, pathToWord2VecPretrained, df):\n",
    "        tokenizer = pickle.load(open(self.pathToTokenizer, 'rb'))\n",
    "        embed_dim = int(self.pathToTokenizer.split(\"Freq-\")[1])\n",
    "        if \"WRul\" in self.pathToTokenizer:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0]) - 18\n",
    "        else:\n",
    "            MAX_SEQUENCE_LENGTH = int(self.pathToTokenizer.split(\"Seq-\")[1].split(\"_\")[0])\n",
    "\n",
    "        sequences = tokenizer.texts_to_sequences(df['tweet_text'])\n",
    "        X_train_WoR_dtm = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        X_train_Features = [\"_Not_Apply_\"]\n",
    "        # print(type(X_train_WoR_dtm), X_train_WoR_dtm.shape)\n",
    "        if self.include_rules:\n",
    "            df_notPreprocessed = pd.read_csv(self.path_to_nonPreprocessedDataset)\n",
    "            X_train_dtm, _ = self.apply_gen_rules_features(df_notPreprocessed[\"tweet_text\"], X_train_WoR_dtm, tokenizer, \\\n",
    "                                                      X_train_Features)\n",
    "        else:\n",
    "            X_train_dtm = X_train_WoR_dtm\n",
    "        inv_tokenizer = self.invTokenizer(tokenizer)\n",
    "        fun_word_to_vec_map = self.load_word2vec_model(pathToWord2VecPretrained)\n",
    "        vector = []\n",
    "        for tw_sequence in X_train_dtm:\n",
    "            vec = self.word_vector_avg(inv_tokenizer, fun_word_to_vec_map, tw_sequence, embed_dim)\n",
    "            vector.append(vec)\n",
    "        X_train_dtm = np.array(vector)\n",
    "        return X_train_dtm\n",
    "    \n",
    "    def load_word2vec_model(self, path):\n",
    "        if \"Model\" not in path:\n",
    "            print(\"You may not loading a model; You may loading a tokenizer.\")\n",
    "        if \"glove\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"word2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"google\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        elif \"doc2vec\".lower() in path.lower().split(\"/\")[-1]:\n",
    "            word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "            \n",
    "        elif \"crisisNLP2vec\".lower() in path.lower().split(\"/\")[2]:\n",
    "             word_to_vec_map = pickle.load(open(path, 'rb'))\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    def get_predictions(self, clf_model, X):\n",
    "        return clf_model.predict(X)\n",
    "    \n",
    "    def generateNameFromDataSetName(self, list1):\n",
    "        str1 = \"\"\n",
    "        for e in list1:\n",
    "            if e == \"\":\n",
    "                str1 += '.'.join(str(e))\n",
    "            else:\n",
    "                if e == \"csv\":\n",
    "                    e = \"_withPredictions.\" + e\n",
    "                    str1 += ''.join(str(e))\n",
    "                else:\n",
    "                    str1 += ''.join(str(e))\n",
    "        return \"..\" + str1\n",
    "\n",
    "    def generatePredWord2VecPretrained(self, path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                     pathToClfModel, path2word2vecModel, pathToWord2VecPretrained):\n",
    "        NotProcessedDataWithPredictions = pd.read_csv(obj.path_to_nonPreprocessedDataset, encoding = \"ISO-8859-1\")\n",
    "        df = pd.read_csv(obj.path_to_PreprocessedDataset,  encoding = \"ISO-8859-1\")\n",
    "        for vect_filename in glob.glob(pathToVectFolder + \"/*\"):\n",
    "            print(40*\"+\")\n",
    "            obj.traditional_ChangeVectPath(vect_filename)\n",
    "            Vect = obj.traditional_loadVect()\n",
    "            for pretrainedWord2VecModel_filename in glob.glob(pathToWord2VecPretrained + \"*\"): \n",
    "                if pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\") == \\\n",
    "                vect_filename.split(\"/\")[-1].split(\"_\"):\n",
    "                    obj.word2vec_ChangeWord2VecPretrainedPath(pretrainedWord2VecModel_filename)\n",
    "                    for clfModel_filename in glob.glob(pathToClfFolder + \"*\"):\n",
    "                        if clfModel_filename.split(\"/\")[-1].split(\"_\")[6:-1] == pretrainedWord2VecModel_filename.split(\"/\")[-1].replace(\"Model\", \"Vect\").split(\"_\"):\n",
    "                            obj.traditional_ChangeClfModelPath(clfModel_filename)\n",
    "                            name_cell = obj.pathToClfModel.split(\"/\")[-1].split(\"_\")[5:]\n",
    "                            if name_cell[-3] == \"WRul\":\n",
    "                                obj.include_rules = True\n",
    "                            else:\n",
    "                                obj.include_rules = False\n",
    "                            X = obj.word2vec_Vect(pretrainedWord2VecModel_filename, df)\n",
    "                            clf_model = obj.traditional_loadClf()\n",
    "                            y_pred = obj.get_predictions(clf_model, X)\n",
    "                            name_cell = name_cell[-6] + \"_\" + name_cell[-3]  + \"_\" + name_cell[-4] + \"_\" + name_cell[-2] + \"_\" + name_cell[-1]\n",
    "                            print(name_cell)\n",
    "                            NotProcessedDataWithPredictions[name_cell] = y_pred.tolist()\n",
    "        notProcessedDatasetName = obj.generateNameFromDataSetName(obj.path_to_nonPreprocessedDataset.split(\".\"))\n",
    "        NotProcessedDataWithPredictions.to_csv(notProcessedDatasetName)\n",
    "        print(40*\"-\")\n",
    "    \n",
    "\n",
    "pathToClfModel = \"\" \n",
    "path_to_nonPreprocessedDataset = \"../datasets/500_random_sample.csv\"\n",
    "path_to_PreprocessedDataset = \"../datasets/500_random_sample_processed.csv\"\n",
    "pathToVectFolder = \"../word2vec_vect/\"\n",
    "pathToClfFolder = \"../word2vec_clfModels/\"\n",
    "\n",
    "\n",
    "path2word2vecModel = 'dummyPath/word2vecModels/word2vec'\n",
    "pathToWord2VecPretrained = \"../word2vec_preTrainedModel/\"\n",
    "obj = Use_Word2Vec(pathToVectFolder, pathToClfModel, pathToWord2VecPretrained, path_to_PreprocessedDataset, \\\n",
    "                   path_to_nonPreprocessedDataset)\n",
    "obj.generatePredWord2VecPretrained(path_to_nonPreprocessedDataset, path_to_PreprocessedDataset, pathToVectFolder,\\\n",
    "                                 pathToClfModel, path2word2vecModel, pathToWord2VecPretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba734536-4d2d-442e-a0f0-e64fb3fd9ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22-05-13_17-07-57_word2vec_Model_WRul_Max-Len-Seq-38_Freq-300'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrainedWord2VecModel_filename.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b401d-fd53-4de2-a796-10f2addc4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "../word2vec_clfModels/_DT ../word2vec_vect/22-05-06_22-56-41_word2vec_Model_WoRul_Max-Len-Seq-26_Freq-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e5584-07aa-4927-bbdc-3c89edc4498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[22-05-06_22-56-41_Vect_WoRul_Max-Len-Seq-26_Freq-300_22-05-06_22-56-41_word2vec_Vect_WoRul_Max-Len-Seq-26_Freq-300]\n",
    "==\n",
    "['22-05-06', '22-56-41', 'word2vec', 'Model', 'WoRul', 'Max-Len-Seq-26', 'Freq-300']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virEnv375",
   "language": "python",
   "name": "virenv375"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

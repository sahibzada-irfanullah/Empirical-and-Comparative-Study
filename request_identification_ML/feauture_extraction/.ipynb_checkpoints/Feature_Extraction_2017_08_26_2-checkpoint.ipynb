{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9016380a0382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from my_libraries.my_XL_Cls import  XL_Results_writing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from my_libraries.my_progressBar import My_progressBar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_progressBar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMy_progressBar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# import time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')#replace ignore with default for enabling the warning\n",
    "# import gc\n",
    "# from my_libraries.my_XL_Cls import  XL_Results_writing\n",
    "# from my_libraries.my_progressBar import My_progressBar \n",
    "from ..utilities.my_progressBar import My_progressBar \n",
    "\n",
    "# import time\n",
    "import scipy\n",
    "import time\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn import neighbors\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from sklearn import ensemble\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import svm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# import pickle\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "import re\n",
    "# import math\n",
    "\n",
    "class Ngrams_Rules_Features:\n",
    "  '''Generating Ngrams and rules features'''\n",
    "  def apply_regex(self, text, regex):\n",
    "    match_found = (re.search(regex ,text) !=None)\n",
    "    match_found = int(match_found == True)\n",
    "    return match_found\n",
    "\n",
    "  def gen_rules_features(self, X_data_series): # , X_data_dtm, features_arg):\n",
    "    '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "    X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "    regexes = [\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I\\'m)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'re)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'ll)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b\\w*\\s*\\b\\?', re.I|re.M),\n",
    "      re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M)\n",
    "\n",
    "    ]\n",
    "    temp = pd.DataFrame()\n",
    "    features_arg = []\n",
    "    for i, regex in zip(range(len(regexes)), regexes):\n",
    "      columnName = \"RegEx_\" + str(i + 1)\n",
    "      features_arg.append(columnName)\n",
    "      temp[columnName] = X_data_DF['tweet-text'].apply(lambda text: self.apply_regex(text, regex))\n",
    "    temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "    return temp_sparse, features_arg\n",
    "\n",
    "  def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "    combined_features = features_X + features_Rules\n",
    "    concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "    return concat_sparse, combined_features\n",
    "\n",
    "  def gen_Ngrams(self, X_train, X_test, lower, higher):\n",
    "    Vect = CountVectorizer(ngram_range=(lower, higher))\n",
    "    X_train_dtm = Vect.fit_transform(X_train)\n",
    "    X_test_dtm = Vect.transform(X_test)\n",
    "    return X_train_dtm, X_test_dtm, Vect.get_feature_names()\n",
    "\n",
    "  def sparse_matrix_to_DataFrame(self, X_data_dtm, features):\n",
    "    X_data_dtm = pd.DataFrame(X_data_dtm.toarray(), columns=features)\n",
    "    return X_data_dtm\n",
    "\n",
    "  def series_to_DataFrame(self, X_data):\n",
    "    X_data = X_data.to_frame()\n",
    "    return X_data\n",
    "\n",
    "\n",
    "  def apply_gen_rules_features(self, X, X_train_WoR_dtm, X_train_WoR_Features):\n",
    "    X_Rules_dtm, features_Rules = self.gen_rules_features(X)\n",
    "    X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm, X_train_WoR_Features, features_Rules)\n",
    "    return  X_train_WR_dtm, combined_features\n",
    "\n",
    "\n",
    "  def simple_CountVect(self, X, lower, higher, minDF, maxDF, include_rules):\n",
    "    Vect = CountVectorizer(ngram_range=(lower, higher), min_df = minDF, max_df = maxDF)\n",
    "    X_train_WoR_dtm = Vect.fit_transform(X)\n",
    "    X_train_Features = Vect.get_feature_names()\n",
    "    if include_rules == 'yes':\n",
    "      X_train_dtm, X_train_Features  = self.apply_gen_rules_features(X, X_train_WoR_dtm, X_train_Features)\n",
    "    else:\n",
    "      X_train_dtm = X_train_WoR_dtm\n",
    "    return X_train_dtm, X_train_Features, X_train_dtm.shape\n",
    "\n",
    "  def tfIdf_Vect(self, X, lower, higher, minDF, maxDF, include_rules):\n",
    "    Vect = TfidfVectorizer(ngram_range=(lower, higher), min_df=minDF, max_df=maxDF)\n",
    "    X_train_WoR_dtm = Vect.fit_transform(X)\n",
    "    X_train_Features = Vect.get_feature_names()\n",
    "    if include_rules == 'yes':\n",
    "      X_train_dtm, X_train_Features  = self.apply_gen_rules_features(X, X_train_WoR_dtm, X_train_Features)\n",
    "    else:\n",
    "      X_train_dtm = X_train_WoR_dtm\n",
    "    return X_train_dtm, X_train_Features, X_train_dtm.shape\n",
    "\n",
    "\n",
    "  def generate_Features(self, X, vect_type = 'simple', minDF = 1, maxDF = 1.0, include_rules = 'yes'):\n",
    "    '''Generating Features from data\n",
    "    -Takes attributes,responses, vect_type(simple by default), minDF, maxDF'''\n",
    "    #generating name for the dataset file\n",
    "    new_Feat = ''\n",
    "    if vect_type == 'simple':\n",
    "      new_Feat = '_CountVect'\n",
    "      func_handler = self.simple_CountVect\n",
    "    else:\n",
    "      new_Feat = '_TfIdfVect'\n",
    "      func_handler = self.tfIdf_Vect\n",
    "    new_Feat = new_Feat +'_minDF' + str(minDF) + '_maxDF' + str(maxDF)\n",
    "    if include_rules == 'yes':\n",
    "      new_Feat = new_Feat + '_WRul'\n",
    "    else:\n",
    "      new_Feat = new_Feat + '_WoRul'\n",
    "    pBar = My_progressBar('Generating Features:',6)\n",
    "    for i in range(1, 4):\n",
    "      for j in range(i, 4):\n",
    "        start_time = time.time()\n",
    "        if i == j == 1:\n",
    "          X_train_dtm_uni, X_train_Features_uni, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniLabel = \"Unigrams\" + new_Feat + '_Freq' + str(dim[1])\n",
    "        elif (i == 1) & (j == 2):\n",
    "          X_train_dtm_uniBi, X_train_Features_uniBi, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniBiLabel = \"UniAndBigrams\"+ new_Feat + '_Freq' + str(dim[1])\n",
    "        elif (i == 1) & (j == 3):\n",
    "          X_train_dtm_uniBiTri, X_train_Features_uniBiTri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniBiTriLabel = \"UniBiAndTrigrams\" + new_Feat + '_Freq' + str(dim[1])\n",
    "        elif (i == 2) & (j == 2):\n",
    "          X_train_dtm_bi, X_train_Features_bi, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          biLabel = \"Bigrams\"+ new_Feat + '_Freq' + str(dim[1])\n",
    "        elif (i == 2) & (j == 3):\n",
    "          X_train_dtm_biTri, X_train_Features_biTri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          biTriLabel = \"BiAndTri-grams\" + new_Feat + '_Freq' + str(dim[1])\n",
    "        elif (i == 3) & (j == 3):\n",
    "          X_train_dtm_tri, X_train_Features_tri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          triLabel = \"Trigrams\" + new_Feat + '_Freq' + str(dim[1])\n",
    "        pBar.call_to_progress(start_time)\n",
    "    encap = {'unigrams':{'instances':X_train_dtm_uni, 'header':X_train_Features_uni, 'specs': uniLabel},\\\n",
    "          'uniBigrams':{'instances':X_train_dtm_uniBi, 'header':X_train_Features_uniBi, 'specs': uniBiLabel},\\\n",
    "          'uniBiTrigrams':{'instances':X_train_dtm_uniBiTri, 'header':X_train_Features_uniBiTri, 'specs': uniBiTriLabel},\\\n",
    "          'bigrams':{'instances':X_train_dtm_bi, 'header':X_train_Features_bi, 'specs': biLabel},\\\n",
    "          'biTrigrams':{'instances':X_train_dtm_biTri, 'header':X_train_Features_biTri, 'specs': biTriLabel},\\\n",
    "          'trigrams':{'instances':X_train_dtm_tri, 'header':X_train_Features_tri, 'specs': triLabel}}\n",
    "    return encap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "script_start = time.time() #script start point\n",
    "# fnameTrainingDataset = \"/home/sahibzada/Desktop/ipythonNB/ThImp/Datasets/specific labelled/preprocessed_dataset_specific.csv\"\n",
    "fnameTrainingDataseet = \"/home/sahibzada/Desktop/ipythonNB/ThImp/Datasets/TestingDatasets/preprocessed_dataset.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# dataset['tweet-class'] = dataset['tweet-class'].map({'request': 1, 'not_excl': 0})\n",
    "df = pd.read_csv(fnameTrainingDataseet)\n",
    "X = df['tweet-text']\n",
    "y = df['tweet-class']\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "feat_extract = Ngrams_Rules_Features()\n",
    "features = feat_extract.generate_Features(X,'simple', 1, 1.0,'yes')\n",
    "# print(features['unigrams']['specs'])\n",
    "# print(features['uniBigrams']['specs'])\n",
    "# print(features['uniBiTrigrams']['specs'])\n",
    "# print(features['bigrams']['specs'])\n",
    "# print(features['biTrigrams']['specs'])\n",
    "# print(features['trigrams']['specs'])\n",
    "# mat = features['unigrams']['instances']\n",
    "# head = features['unigrams']['header']\n",
    "# data = pd.DataFrame(mat.toarray(), columns = head)\n",
    "# print(data)\n",
    "\n",
    "print(\"\\nscritp completed\")\n",
    "Total_time = time.time() - script_start\n",
    "print(\"\\nTotal time for script completion :\" + str(datetime.timedelta(seconds=int(Total_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['unigrams']['instances'].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipythonNB",
   "language": "python",
   "name": "ipythonnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
